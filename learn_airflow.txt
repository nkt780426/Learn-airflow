Nguồn học: https://www.youtube.com/watch?v=K9AnJ9_ZAXE&t=234s

1. Airflow Introduction
2. Run Airflow in Python Env (không dùng, thích có thể xem cài đặt trên Linux)
3. Run Airflow in Docker (Bài này dùng)
- Đọc cách tạo airflow bằng docker compose: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

- start project
    docker compose up airflow-init && docker compose up -d
- stop project:
    docker compose down -v  (-v nghĩa là bỏ cả volumn được tạo ra)

4. Airflow Basics and Core Concepts
Airflow là gì ?
- Được phát triển bởi Airbnb vào năm 2014 để manage lượng complex workflow ngày càng tăng
- Trở thành project của Apache vào March 2016 và trở thành top-level của công ty apache vào January 2019
- Là workflow management platforms phổ biến nhất
- Được viết bằng python
Workflow là gì ?
- Workflow là 1 khái niệm mindset chỉ cách thực hiện 1 công việc nào đó. 
- Được định nghĩa trong lập trình bằng DAG. Trong 1 DAG có thể có rất nhiều các task và được sắp xếp 1 cách tuần tự thứ tự thực hiện
- Trong airflow, workflow được xác định trong thư mục dags (directed acyclic graph)
- Ví dụ: (hình 4.workflow)
    Task A được thực hiện xong sẽ thực hiện song song 2 task B và C sau đến D, E => Sự tuần tự thực hiện các task trong 1 workflow vừa nói (không thể thực hiện task B và C xong mới thực hiện task A)
    Các tasks dependence nhau
Dag, Task, Operator là gì ? (hình 4. workflow)
- Task:
    Task là 1 khái niệm mindset
    Khi làm việc 1 workflow, 1 suy nghĩ phổ biến là module hóa nó thành các task nhỏ hơn để dễ dàng thực hiện => Task là 1 unit của workflow/DAG (được đại diện bởi 1 node trong đồ thị DAG)
    C được gọi là downstream của task A
    C được gọi là upstream của task E
- Operator:
    Task chỉ là khái niệm mindset, để thực thị 1 task trong airflow ta cần sử dụng các operator
    Có rất nhiều loại Operator trong airflow như BashOperator, PythonOperator, ... (thậm chí có thể customize 1 Operator mới)
    BashOperator: Thực thi các câu lệnh Bash
    PythonOperator: thực thi các câu lệnh Python
Execution Date, Task Instance, Dag run là gì ? (hình 4. instance)
    Dag, task, operator ở trên chỉ là class, còn mấy cái khái niệm này là instance
    Excution date là thời gian để chạy dag 1 cách tự động, chỉ cần lên lịch thì dag sẽ tự động chạy (lý do cần airflow)
    Task instance đại diện cho task tại mỗi thời điểm thực thi Dag khác nhau
    Dag run đại diện cho dag tại mỗi thời điểm chạy khác nhau

5. Airflow Task Lifecycle (hình 5. task_status, 5. task_life_cycle) (chính ra là workflow Lifecycle)

- Workflow ban đầu ở trạng thái "No status": Scheduler tạo ra các empty task và lên lích thực hiện các task 1 cách sequence như phần 4 đã nói
    "scheduled": các task trong workflow/dag được lên lịch thành công
    "removed": nếu bị scheduled bỏ
    "upstream" failed: khi upstream của task failed
    "skipped": nếu task không được thực thi
- Dag nếu may mắn sau bước trên sẽ ở trạng thái "scheduled" và chuyển đến cho excutor.
    "queue": task được excutor ném vào queue chờ thưc thị, lúc này staus của workflow chuyển sang "queue"
    "running": worker sẽ thực thi task nếu nó có tài nguyên đủ dùng, lúc này staus workflow chuyển sang "runing"
    Dựa vào kết quả worker thực thi task, task sau cùng sẽ có các trạng thái "success", "failed", "shutdown" (bắt đầu bị hủy bỏ)
- Worker sẽ cố gắng retry lại task nếu nó "failed" hoặc "shutdown", lúc này task sẽ ở trạng thái "up for retry" và được đựa lại cho scheduler lên lịch
- Trong 1 vài trường hợp, khi task ở trạng thái "running" có thể được chuyển đến trạng thái "up to reschedule" nghĩa là task sẽ được lặp lại trong 1 khoảng thời gian nào đó

=> Tóm lại: 1 workflow tốt sẽ bắt đầu với "no staus" sau đó được scheduled lên lịch các task và được excutor đặt các task vào trong queue chờ worker lấy ra thực thi. Worker sẽ thực thi thành công tất cả các task và workflow sẽ ở trạng thái "success"

6. Airflow Basic Architecture (6. architech)

- Data Engineer: người build và mornitoring tất cả ELT proceess => Dùng tool để quản lý ELT script là airflow
- Để config được airflow cần phải config các thành phần như:
    Loại excutor của airflow, loại database sẽ sử dụng
- Data Engineer sẽ tạo và quản lý các workflow/dag thông qua User Interface (giao diện) của airflow
    Giao điện được support bởi web server
- Dags phải được nhìn thấy bởi các tiến trình như scheduler, workers
- Ngoài các thành phần trên trong airflow còn có excutor dùng để lưu trữ những update hoặc lấy lại thông tin của dags
=> Tất cả 4 thành phần (web server, scheduler, executor, workers) đều phải liên kết với 1 database nào đó như postgresql, mysql, sqllite, ...

7. Airflow DAG with Bash Operator

- Trong docker compose clone về từ hãng, chỉnh lại biến môi trường sau để không load các example mẫu
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
- Xem file first_dag.py trong thư mục dags

8. Airflow DAG with Python Operator

- Xem file create_dag_with_python_opertor.py

9. Data Sharing via Airflow XComs

Có thể chia sẻ thông tin giữa 2 task với nhau trong cùng 1 workflow/dag không ?
- Sử dụng Xcoms
- Xcoms như là volume trong docker, lưu trữ dữ liệu dưới dạng key-value trong DB của airflow
- Mỗi Xcoms có 1 key (tên dữ liệu), value (dữ liệu thực tế), một task_id (ID của nhiệm vụ tạo ra Xcom) và 1 dag_id(ID của DAG chứa task đó)
- Mối task có thể được push/pull vào xcom bằng hàm "xcom_push"/"xcom_pull"
- Thường sử dụng PythonOperator để thao tác với Xcoms
Xem file xcoms_dag.py
Theo mặc định các hàm return lại value sẽ được lưu vào xcom. và các hàm khác có thấy lấy value từ Xcoms
Xcoms chỉ có max size là 48Kb => Không dùng Xcoms để chia sẽ dữ liệu lớn

10. Airflow Task Flow API

Qua việc thực hành các file "create_dag_with_python_operator.py", "our_first_dag.py", "xcom_dag.py" ta có thể thấy code các task rất rõ ràng, tuy nhiên khi số lượng task lớn lên, việc tự tay thiết lập logic dag trở lên khó khăn và có thể không tối ưu => Taskflow API là 1 API của airflow giúp chúng ta tự động cấu hình workflow các task sao cho tối ưu nhất
Xem file "dag_with_task_flow_api.py"

11. Airflow Catch-Up and Backfill

Catchup: Khi thiết lập dag, ta cấu hình startdate. Tham số catchup trong một DAG quyết định xem Airflow có nên tự động chạy tất cả các lần chạy đã bị bỏ lỡ từ start_date cho đến ngày hiện tại khi DAG được kích hoạt hoặc bật lên lần đầu tiên hay không.
Backfill là quá trình chạy lại các task của một DAG cho các khoảng thời gian trong quá khứ mà DAG đã bị bỏ lỡ hoặc chưa được thực thi. Điều này thường được sử dụng khi có thay đổi trong DAG hoặc dữ liệu nguồn và cần phải cập nhật lại kết quả cho các khoảng thời gian trước đó.

12. Airflow Scheduler with Cron Expression

Cross Expression (xem hình 12. cross_expression.png)
Airflow cung cấp 1 vài cross_expression có sẵn (xem hình 12. airflow_cross_expression)
- Sử dụng trang crontab guru để sinh cross expression phù hợp
Xem file dag_with_cross_expression.py

13. Airflow Connection to Postgres

Khi viết script ELT, hiển nhiên phải có nhu cầu kết nối đến nhiều source để thu thập dữ liệu (hình 13. collection)
- Để kết nối với các thành phần này, nhiều lúc bạn cần phải cung cấp username/password/ip_host/...
- Airflow collection được thực hiện bởi corresponding operator (là tập hợp các operator làm mục đích kết nối đọc phần 14, 15, 16, 17)
- Xem file dag_with_postgres_operator.py

14. Airflow Postgres Operator

Xem file dag_with_postgres_operator.py

15. Airflow Docker Install Python Package 2 ways

Để cài đặt dependence cho airflow container có 2 cách
- Extension airflow image mà hãng cung cấp (hầu hết đều theo cách này)
- Cunstom airflow image
Extension airflow image
- B1: tạo file requirements.txt ở thư mục root của project
- B2: tạo Dockerfilư ở thư mục root của project
    Build extending image: docker build . --tag extending_airflow:lastest
- B3: Thay thế image trong docker compose của hãng
        ---
        x-airflow-common:
        &airflow-common
        # In order to add custom dependencies or upgrade provider packages you can use your extended image.
        # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
        # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
        image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}
    => Thay image: apache/airflow:2.9.2 thành extending_airflow:lastest

16. Airflow AWS S3 Sensor Operator

Sensor operator là 1 loại operator đặc biệt, nó sẽ được kích hoạt khi có sự kiện nào đó xảy ra (ví dụ khi update 1 file csv trên s3)
Cài MinIO:
- Cài theo phương thức Docker root full và set up thư mục data trong project này thay vì ~/minio/data
    mkdir -p ./minio/data
    docker run \
    -p 9000:9000 \
    -p 9001:9001 \
    --name minio \
    -v ./minio/data:/data \
    -e "MINIO_ROOT_USER=ROOTNAME" \
    -e "MINIO_ROOT_PASSWORD=CHANGEME123" \
    quay.io/minio/minio server /data --console-address ":9001"
- Tạo airflow bucket và kéo file csv mẫu trong thư mục data vào bucket
- Vào container scheduler chạy lệnh sau để tìm version của apache-airflow-providers-amazon
    pip list | grep amazon
    
- Lưu ý, AWS S3 connection đã bị removed và thay bằng https://www.youtube.com/watch?v=sVNvAtIZWdQ&t=184s (chú ý cái endpoint)
17. Airflow Hooks S3 PostgreSQL

Airflow Hook được sử dụng để tạo 1 kết nối với các hệ thống và dịch vụ bên ngoài như CSDL và các hệ thống lưu trữ đám mây khác. Hooks cung cấp 1 lớp trừu tượng để dễ dàng giao tiếp với các dịch vụ bên ngoài mà không cần phải viết mã kết nối từng loại cho chúng

18. DockerOperator
- Cách fix lỗi: https://github.com/apache/airflow/issues/16803
=> sudo chmode 777 /var/run/docker.sock
    Thêm 2 volume 
        '/var/run/docker.sock:/var/run/docker.sock'
        '/tmp:/tmp'
    Kích hoạt bật xác thực không TLS
        Docker desktop => General => Bật TLS tcp:2375 gì đó

Phần tiếp theo của khóa học 2 tiếng trên: https://www.youtube.com/watch?v=uZy2Lwioi3g&t=336s
DockerOperator sẽ tun task bên trong Docker container thay vì PythonOperator như ở trên
Tải sao cần DockerOpertor ?
- Code isolation: nếu sử dụng PythonOperator sẽ phải định nghĩa các python function trong DAG, điều này sẽ tốt nếu workflow của bạn đơn giản. Tuy nhiên khi đối đầu với complex logic, cách tốt nhất là chia project ra thành các phần độc lập với nhau để dễ bảo trì
- Sử dụng container giúp cải thiện khả năng tái sử dụng và mở rộng hệ thống bằng cách đóng gói lại trong Docker images

docker build -t regression-training-image:v1.0 .

docker run \
    -e MINIO_ENDPOINT=host.docker.internal:9000 \
    -e MINIO_ACCESS_KEY_ID=5u9G26HTEpIUjVK3Kx5U \
    -e MINIO_SECRET_ACCESS_KEY=WomfMRMWNnhISl7v0SOCa2nXIDEkbYAzAuhYF2ug \
    -e MINIO_BUCKET_NAME=airflow \
    regression-training-image:v1.0

19. Airflow vs Celery
Có nhiều loại executor trong airflow. Project chạy với celery executor
    1. SequentialExecutor (không bao h dùng)
    - Chạy 1 task tại 1 thời điểm
    - Thích hợp cho môi trường development và testing, không dùng trong môi trường production
    2. LocalExcutor (dùng trong môi trường development)
    - Chạy các task đồng thời trong các tiến trình riêng biệt trên cùng 1 máy chủ
    - Thích hợp cho môi trường development và môi trường production phải chạy nhiều task nhưng không cần 1 hệ thống phân tán phức tạp
    3. CeleryExecutor (dùng trong môi trường production)
    - Sử dụng Celery để phân phối và xử lý các task. Celery là một hệ thống phân tán cho phép thực hiện các tác vụ song song.
    - Thích hợp cho môi trường production với quy mô lớn hơn, nơi cần phân phối và quản lý việc thực thi task trên nhiều worker.
    - Có khả năng mở rộng tốt với nhiều worker
    - Cần phải cấu hình thêm Redis hoặc RabbitMQ làm broker và 1 backend để lưu trữ kết quả
    4. DaskExecutor
    - Sử dụng Dask, một thư viện Python cho tính toán phân tán, để thực thi các task.
    - Thích hợp cho các công việc tính toán phức tạp yêu cầu khả năng mở rộng và phân tán, đặc biệt khi các task cần xử lý khối lượng dữ liệu lớn.
    - Hỗ trợ tính toán phân tán và có thể tích hợp với các công cụ phân tích dữ liệu khác.
    5. KubernetesExecutor
    - Chạy các task trong các pod Kubernetes riêng biệt. Mỗi task sẽ chạy trong một pod độc lập.
    - Thích hợp cho các môi trường sử dụng Kubernetes, giúp tận dụng khả năng mở rộng và quản lý tài nguyên của Kubernetes.

Overview toàn bộ về [celery](https://www.youtube.com/watch?v=hFIkEGtW6vE).
- Giả sử có 1 api backend thuộc dạng POST, tại đó yêu cầu server generate 1 csv file và trả về kết quả thành công hay không. Có thể nhận thấy api sẽ bị block tại hàm generate_csv_report.

![block api](images/19.celery_block_api).

- Giải pháp là sử dụng 1 machine khác với vai trò worker để thực hiện các task từ queue. Trong đó cần thêm 1 broker có thể là RabbitMQ, redis hay AWS SQS. Khi thực hiện đến hàm generate_csv_report, thay vì block request và thực hiện nó, ta ném nó vào broker để worker thực hiện, còn server chỉ cần return về kết quả. Làm vậy sẽ không bị block.

![async api](images/19.celery_block_api).

![celery lib](images/19.celery_lib.png).

Celery bản chất là 1 giải pháp để tạo producer và consumer của 1 broker nào đó. Để sử dụng producer có thể publish lên broker, sử dụng hàm delay(). Để worker sử dụng lib để định nghĩa cách xử lý các task có thể nhận được từ broker.

Celery thường được sử dụng trong các hoạt động như send email, notification (thông báo), generate_report, ...

Worker của celery có 1 process pool riêng như task slot trong flink hay thread pool. Bạn phải khai báo số lượng process tối đa 1 worker có thể xử lý

Celery có thể lên lịch tạo các task đẩy vào queue bằng cách tạo 1 celery beat.

![celery beat](images/19.celery_hear_beat).

Theo mặc định khi dùng [docker-compose](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html) airflow mặc định thì celery excutor sẽ đuọc sử dụng theo mặc định. Nó có các service sau
    1. postgres: nơi lưu trữ metadata của cả airflow (dags, logs, plugin, ...)
    2. redis: cung cấp dịch vụ redis dùng như broker cho Celery Excutor
    3. airlflow-webserver: Cung cấp giao diện web của Airflow để quản lý và giám sát các DAGs.
    4. airflow-scheduler: Điều phối và lên lịch thực hiện các DAGs và tasks trong Airflow.
    5. airflow-worker: Thực thi các task của Airflow khi sử dụng Celery Executor.
    6. airflow-triggerer: Xử lý các trigger job, để xử lý các DAGs cần phải được kích hoạt.
    7. airflow-init: Thực hiện các bước khởi tạo cần thiết trước khi các dịch vụ chính của Airflow được khởi động, chẳng hạn như tạo người dùng quản trị và thiết lập cơ sở dữ liệu.
    8. airflow-cli: Cung cấp một container để thực thi các lệnh Airflow từ dòng lệnh.
    9. flower: Cung cấp giao diện web để giám sát và quản lý các worker của Celery. (mở port 5555 để nhiìn giao diện flower)